<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Wilson Yan</title>

  <meta name="author" content="Wilson Yan">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Wilson Yan</name>
              </p>
              <p>
                Hi everyone! I'm a senior research scientist at Google DeepMind and a 4th year Computer Science PhD Student at UC Berkeley. I am currently advised by <a href="https://people.eecs.berkeley.edu/~pabbeel/">Professor Pieter Abbeel</a> as a part of <a href="https://bair.berkeley.edu/">Berkeley AI Research</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:wilson1.yan@berkeley.edu">Email</a> &nbsp/&nbsp
<!--                 <a href="data/WilsonYanResume.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=tR2Qw0YAAAAJ&hl=en">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/WilsonYan.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/WilsonYan.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am interested in generative modeling. My current research is focused on learning models for accurate video prediction and generation.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='lwm.png'><img src='images/lwm.png'></div>
                <img src='images/lwm.png'>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2402.08268">
                <papertitle>World Model on Million-Length Video and Language With Blockwise RingAttention</papertitle>
              </a>
              <br>
              <a href="https://www.haoliu.site/">Hao Liu*</a>,
              <a href="https://wilson1yan.github.io">Wilson Yan*</a>,
              <a href="https://people.eecs.berkeley.edu/~matei/">Matei Zaharia</a>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>
              <br>
              <em>In Submission</em>
              <br>
              <a href="https://largeworldmodel.github.io/">project page</a>
              <br>
              <a href="https://twitter.com/haoliuhl/status/1757828392362389999">tweet</a>
              <br>
              <p></p>
              <p>A vision-language model trained on million-length sequences of images, language, and video.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='MoCA.png'><img src='images/MOCA.png'></div>
                <img src='images/MoCA.png'>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2311.18827">
                <papertitle>Motion-Conditioned Image Animation for Video Editing</papertitle>
              </a>
              <br>
              <a href="https://wilson1yan.github.io">Wilson Yan</a>,
              <a href="https://www.robots.ox.ac.uk/~abrown/">Andrew Brown</a>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
              <a href="https://rohitgirdhar.github.io/">Rohit Girdhar</a>,
              <a href="https://scholar.google.com/citations?user=X0EXfT8AAAAJ&hl=en">Samaneh Azadi</a>
              <br>
              <em>In Submission</em>
              <br>
              <a href="https://facebookresearch.github.io/MoCA/">project page</a>
              <br>
              <a href="https://twitter.com/wilson1yan/status/1730661144803897789">tweet</a>
              <br>
              <p></p>
              <p>A motion-conditioned image animation approach to video editing that enabled a diverse range of edit types</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='VIPER.png'><img src='images/VIPER.png'></div>
                <img src='images/VIPER.png'>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2305.14343">
                <papertitle>Video Prediction Models as Rewards for Reinforcement Learning</papertitle>
              </a>
              <br>
              <a href="https://www.escontrela.me/">Alejandro Escontrela*</a>,
              <a href="https://ademiadeniji.github.io/">Ademi Adeniji*</a>,
              <a href="https://wilson1yan.github.io">Wilson Yan*</a>,
              <a href="https://www.ajayjain.net/">Ajay Jain</a>,
              <a href="https://xbpeng.github.io/">Xue Bin Peng</a>,
              <a href="https://goldberg.berkeley.edu/">Ken Goldberg</a>,
              <a href="https://youngwoon.github.io/">Youngwoon Lee</a>,
              <a href="https://danijar.com/">Danijar Hafner</a>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>
              <br>
              <em>NeurIPS 2023</em>
              <br>
              <a href="https://www.escontrela.me/viper/">project page</a>
              <br>
              <a href="https://twitter.com/AleEscontrela/status/1661363555495710721">tweet</a>
              <br>
              <p></p>
              <p>We show that video prediction likelihoods can provide more dense reward signal for learning reinforcement learning agents</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='LQAE.png'><img src='images/LQAE.png'></div>
                <img src='images/LQAE.png'>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2302.00902">
                <papertitle>Language Quantized AutoEncoders: Towards Unsupervised Text-Imge Alignment</papertitle>
              </a>
              <br>
              <a href="https://haoliu.site/">Hao Liu</a>,
              <a href="https://wilson1yan.github.io">Wilson Yan</a>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>
              <br>
              <em>NeurIPS 2023</em>
              <br>
              <a href="https://twitter.com/haoliuhl/status/1625273748629901312">tweet</a>
              <br>
              <p></p>
              <p>We learn an aligned text-image token representation without text-image data, that can then be used for downstream VQA and classification tasks using in-context learning with an LLM</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='TECO.png'><img src='images/TECO.png'></div>
                <img src='images/TECO.png'>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2210.02396">
                <papertitle>Temporally Consistent Transfomers for Video Generation</papertitle>
              </a>
              <br>
              <a href="https://wilson1yan.github.io">Wilson Yan</a>,
              <a href="https://danijar.com/">Danijar Hafner</a>,
              <a href="https://stepjam.github.io/">Stephen James</a>
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
              <br>
              <em>ICML 2023</em>
              <br>
              <a href="https://wilson1yan.github.io/teco/index.html">project page</a>
              <br>
              <a href="https://twitter.com/WilsonYan8/status/1578424234727964682?s=20&t=lDRkF1kd5XQBjqVTc9ltOg">tweet</a>
              <br>
              <p></p>
              <p>A vector-quantized latent dynamics video prediction model that learns compressed representations to efficiently condition on long videos of hundreds of frames during both training and generation</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='POVT.png'><img src='images/POVT.png'></div>
                <img src='images/POVT.png'>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2206.04003">
                <papertitle>Patch-based Object-centric Transformers for Efficient Video Generation</papertitle>
              </a>
              <br>
              <a href="https://wilson1yan.github.io">Wilson Yan</a>,
              <a href="https://www.linkedin.com/in/ryo-okumura-408010119/">Ryo Okumura</a>,
              <a href="https://stepjam.github.io/">Stephen James</a>
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
              <br>
              <a href="https://sites.google.com/view/povt-public">project page</a>
              <br>
              <p></p>
              <p>An video generation architecture that leverages an object-centric transformer design for more scalable and efficient video generation</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='VideoGPT.png'><img src='images/VideoGPT.png'></div>
                <img src='images/VideoGPT.png'>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2104.10157">
                <papertitle>VideoGPT: Video Generation using VQ-VAE and Transformers</papertitle>
              </a>
              <br>
              <a href="https://wilson1yan.github.io">Wilson Yan*</a>,
              <a href="https://zzyunzhi.github.io/">Yunzhi Zhang*</a>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
              <a href="https://people.eecs.berkeley.edu/~aravind/">Aravind Srinivas</a>
              <br>
              <a href="https://wilson1yan.github.io/videogpt/index.html">project page</a>
              <br>
              <p></p>
              <p>An efficient and scalable video generation architecture that first learns a VQ-VAE to compress video data, and then learns a GPT-style transformer to model discrete latent codes</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='rsz_contrastive_mpc.png'><img src='images/rsz_contrastive_mpc.png'></div>
                <img src='images/rsz_contrastive_mpc.png'>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2003.05436">
                <papertitle>Learning Predictive Representations for Deformable Objects Using Contrastive Estimation</papertitle>
              </a>
              <br>
              <a href="https://wilson1yan.github.io">Wilson Yan</a>,
              <a href="https://www.linkedin.com/in/ashwin-vangipuram-06606116a/">Ashwin Vangipuram</a>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
              <a href="https://cs.nyu.edu/~lp91/">Lerrel Pinto</a>
              <br>
              <em>CoRL 2020</em>
              <br>
              <a href="https://sites.google.com/view/contrastive-predictive-model/home">project page</a>
              <br>
              <a href="https://bair.berkeley.edu/blog/2020/05/05/fabrics/">blog post</a>
              <br>
              <p></p>
              <p>Using contrastive methods to learn plannable representations for manipulating deformable objects such as rope and cloth</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='rsz_fisher_score.png'><img src='images/rsz_fisher_score.png'></div>
                <img src='images/rsz_fisher_score.png'>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1912.05015">
                <papertitle>Natural Image Manipulation for Autoregressive Models using Fisher Scores</papertitle>
              </a>
              <br>
              <a href="https://wilson1yan.github.io">Wilson Yan</a>,
              <a href="http://www.jonathanho.me/">Jonathan Ho</a>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
              <br>
              <em>Preprint</em>
              <br>
              <p></p>
              <p>Performing image interpolation and semantic manipulation (e.g. hair color, smiling, gender) with a PixelCN on facial images using Fisher scores as an embedding space</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='rsz_1deform_object.png'><img src='images/rsz_1deform_object.png'></div>
                <img src='images/rsz_1deform_object.png'>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1910.13439">
                <papertitle>Learning to Manipulate Deformable Objects without Demonstrations</papertitle>
              </a>
              <br>
              <a href="http://yilinwu.net/">Yilin Wu</a>,
              <a href="https://wilson1yan.github.io">Wilson Yan</a>,
              <a href="http://people.eecs.berkeley.edu/~thanard.kurutach/">Thanard Kurutach</a>,
              <a href="https://cs.nyu.edu/~lp91/">Lerrel Pinto</a>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
              <br>
              <em>RSS 2020</em>
              <br>
              <a href="https://sites.google.com/view/alternating-pick-and-place/home">project page</a>
              <br>
              <a href="https://bair.berkeley.edu/blog/2020/05/05/fabrics/">blog post</a>
              <br>
              <p></p>
              <p>Sim-to-Real transfer to learning a cloth-spreading policy on a real robot with any human demonstrations</p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cs188.jpg" alt="cs188">
            </td>
            <td width="75%" valign="center">
              <b> CS 188: Introduction to Artificial Intelligence </b><br>
              <a href="http://inst.eecs.berkeley.edu/~cs188/fa18">Undergraduate Student Instructor, CS188 Fall 2018</a>
              <br>
              <a href="http://inst.eecs.berkeley.edu/~cs188/sp19">Undergraduate Student Instructor, CS188 Spring 2019</a>
              <br>
              <a href="http://inst.eecs.berkeley.edu/~cs188/fa19">Head Undergraduate Student Instructor, CS188 Fall 2019</a>
              <br>
              <br>
              <b> CS 294-158: Deep Unsupervised Learning </b><br>
              <a href="https://sites.google.com/view/berkeley-cs294-158-sp20/home">Undergraduate Student Instructor, CS294-158 Spring 2020</a>
            </td>
          </tr>
        </tbody></table>

        <a href="https://jonbarron.info/">Website Source</a>
</body>

</html>
